{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for the publication \n",
    "\n",
    "## Validating and Utilizing Machine Learning Methods to Investigate \n",
    "## the Impacts of Synthesis Parameters in Gold Nanoparticle Synthesis\"\n",
    "## by Daniel Schletz, Morten Breidung, and Andreas Fery\n",
    "\n",
    "## CC BY-NC-ND 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57872929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Parameter+DLS\n",
    "a = pd.read_excel(\"Data/Synthesis_Parameters_DLS.xlsx\")\n",
    "#Import Spectra\n",
    "b = pd.read_excel(\"Data/UV-vis_Spectra.xlsx\")\n",
    "#Import Abs_Max Data\n",
    "c = pd.read_excel(\"Data/Absorption_Maximum_FW80M.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0ff498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Parameters\n",
    "X = a.drop(columns=[\"Name\",\"DLS Diameter [nm]\",\"Standard Deviation DLS [nm]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a27b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Begin Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d9eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NestedCV_LR(XData, yData, savetest, savehat):\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    outer_results2 = list()\n",
    "    y_graph_hat = list()\n",
    "    y_graph_test = list()\n",
    "    X2 = XData.to_numpy()\n",
    "    y2 = yData.to_numpy().ravel()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "        X_train, X_test = X2[train_ix, :], X2[test_ix, :]\n",
    "        y_train, y_test = y2[train_ix], y2[test_ix]\n",
    "        y_graph_test.append(y_test)\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = LinearRegression()\n",
    "        # define search\n",
    "        space = dict()\n",
    "        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        y_graph_hat.append(yhat)\n",
    "        # evaluate the model\n",
    "        acc = mean_squared_error(y_test, yhat)\n",
    "        acc2 = r2_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        outer_results2.append(acc2)\n",
    "        # report progress\n",
    "        print('>acc[MSE]=%.3f, acc=%.3f, est=%.3f, cfg=%s' % (acc, acc2, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results2), std(outer_results2)))\n",
    "    # save data\n",
    "    f = open(savetest, 'wb')\n",
    "    pickle.dump(y_graph_test, f)\n",
    "    f.close()\n",
    "    f = open(savehat, 'wb')\n",
    "    pickle.dump(y_graph_hat, f)\n",
    "    f.close()\n",
    "    \n",
    "    return y_graph_test, y_graph_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8838e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NestedCV_RF(XData, yData, savetest, savehat):\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    outer_results2 = list()\n",
    "    y_graph_hat = list()\n",
    "    y_graph_test = list()\n",
    "    X2 = XData.to_numpy()\n",
    "    y2 = yData.to_numpy().ravel()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "        X_train, X_test = X2[train_ix, :], X2[test_ix, :]\n",
    "        y_train, y_test = y2[train_ix], y2[test_ix]\n",
    "        y_graph_test.append(y_test)\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        # define the model\n",
    "        model = RandomForestRegressor(random_state=42, n_jobs = -1)\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['n_estimators'] = [500, 700, 900, 1000, 1100, 1300, 1500]\n",
    "        space['max_features'] = [1.0, 'sqrt', 0.3]\n",
    "        space['min_samples_leaf'] = [1, 2, 4]\n",
    "        space['min_samples_split'] = [2, 4, 8, 16]\n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        y_graph_hat.append(yhat)\n",
    "        # evaluate the model\n",
    "        acc = mean_squared_error(y_test, yhat)\n",
    "        acc2 = r2_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        outer_results2.append(acc2)\n",
    "        # report progress\n",
    "        print('>acc[MSE]=%.3f, acc=%.3f, est=%.3f, cfg=%s' % (acc, acc2, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results2), std(outer_results2)))\n",
    "    # save data\n",
    "    f = open(savetest, 'wb')\n",
    "    pickle.dump(y_graph_test, f)\n",
    "    f.close()\n",
    "    f = open(savehat, 'wb')\n",
    "    pickle.dump(y_graph_hat, f)\n",
    "    f.close()\n",
    "    \n",
    "    return y_graph_test, y_graph_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3aba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NestedCV_AdaB(XData, yData, savetest, savehat):\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    outer_results2 = list()\n",
    "    y_graph_hat = list()\n",
    "    y_graph_test = list()\n",
    "    X2 = XData.to_numpy()\n",
    "    y2 = yData.to_numpy().ravel()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "        X_train, X_test = X2[train_ix, :], X2[test_ix, :]\n",
    "        y_train, y_test = y2[train_ix], y2[test_ix]\n",
    "        y_graph_test.append(y_test)\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(),random_state=42)\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['n_estimators'] = [300, 500]#, 700, 900, 1000, 1100, 1300, 1500]\n",
    "        space['base_estimator__max_depth'] = [None, 6, 8, 10, 12] \n",
    "        space['learning_rate'] = [0.1, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        y_graph_hat.append(yhat)\n",
    "        # evaluate the model\n",
    "        acc = mean_squared_error(y_test, yhat)\n",
    "        acc2 = r2_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        outer_results2.append(acc2)\n",
    "        # report progress\n",
    "        #print('>acc[MSE]=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "        print('>acc[MSE]=%.3f, acc=%.3f, est=%.3f, cfg=%s' % (acc, acc2, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results2), std(outer_results2)))\n",
    "    # save data\n",
    "    f = open(savetest, 'wb')\n",
    "    pickle.dump(y_graph_test, f)\n",
    "    f.close()\n",
    "    f = open(savehat, 'wb')\n",
    "    pickle.dump(y_graph_hat, f)\n",
    "    f.close()\n",
    "\n",
    "    return y_graph_test, y_graph_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f127b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NestedCV_XGB(XData, yData, savetest, savehat):\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    outer_results2 = list()\n",
    "    y_graph_hat = list()\n",
    "    y_graph_test = list()\n",
    "    X2 = XData.to_numpy()\n",
    "    y2 = yData.to_numpy().ravel()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "        X_train, X_test = X2[train_ix, :], X2[test_ix, :]\n",
    "        y_train, y_test = y2[train_ix], y2[test_ix]\n",
    "        y_graph_test.append(y_test)\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        # define the model\n",
    "        model = XGBRegressor(random_state=42, tree_method = 'exact', n_jobs = -1)\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['learning_rate'] = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        space['max_depth']= [6]#None, 6]\n",
    "        space['colsample_bytree'] = [0.5,1.0]\n",
    "        space['colsample_bylevel'] = [0.5,1.0]\n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        y_graph_hat.append(yhat)\n",
    "        # evaluate the model\n",
    "        acc = mean_squared_error(y_test, yhat)\n",
    "        acc2 = r2_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        outer_results2.append(acc2)\n",
    "        # report progress\n",
    "        print('>acc[MSE]=%.3f, acc=%.3f, est=%.3f, cfg=%s' % (acc, acc2, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(outer_results2), std(outer_results2)))\n",
    "    #save data\n",
    "    f = open(savetest, 'wb')\n",
    "    pickle.dump(y_graph_test, f)\n",
    "    f.close()\n",
    "    f = open(savehat, 'wb')\n",
    "    pickle.dump(y_graph_hat, f)\n",
    "    f.close()\n",
    "\n",
    "    return y_graph_test, y_graph_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e55d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc[MSE]=375.941, acc=0.579, est=-3.546, cfg={}\n",
      ">acc[MSE]=505.761, acc=0.385, est=0.036, cfg={}\n",
      ">acc[MSE]=6447.830, acc=-38.284, est=-0.006, cfg={}\n",
      ">acc[MSE]=850.320, acc=0.163, est=-0.084, cfg={}\n",
      ">acc[MSE]=206.338, acc=0.392, est=-5.564, cfg={}\n",
      ">acc[MSE]=424.217, acc=-0.406, est=-0.322, cfg={}\n",
      ">acc[MSE]=153.523, acc=0.524, est=-0.693, cfg={}\n",
      ">acc[MSE]=481.905, acc=0.144, est=-0.293, cfg={}\n",
      ">acc[MSE]=143.292, acc=0.568, est=-1.569, cfg={}\n",
      ">acc[MSE]=166.776, acc=0.203, est=-0.897, cfg={}\n",
      "Accuracy: 975.590 (1835.854)\n",
      "Accuracy: -3.573 (11.574)\n",
      ">acc[MSE]=554.421, acc=0.379, est=0.540, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=283.031, acc=0.656, est=0.482, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=103.211, acc=0.371, est=0.546, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=343.222, acc=0.662, est=0.444, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=198.014, acc=0.417, est=0.480, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=291.516, acc=0.034, est=0.589, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=73.077, acc=0.773, est=0.516, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=332.302, acc=0.410, est=0.561, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=120.907, acc=0.635, est=0.329, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=73.535, acc=0.649, est=0.514, cfg={'min_samples_leaf': 4}\n",
      "Accuracy: 237.324 (145.842)\n",
      "Accuracy: 0.499 (0.207)\n",
      ">acc[MSE]=485.211, acc=0.457, est=0.484, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=272.737, acc=0.668, est=0.313, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=64.789, acc=0.605, est=0.525, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=287.474, acc=0.717, est=0.439, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=187.211, acc=0.449, est=0.358, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=182.421, acc=0.396, est=0.578, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=53.111, acc=0.835, est=0.500, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=338.667, acc=0.399, est=0.457, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=127.444, acc=0.616, est=0.586, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=102.778, acc=0.509, est=0.292, cfg={'n_estimators': 500}\n",
      "Accuracy: 210.184 (129.432)\n",
      "Accuracy: 0.565 (0.140)\n",
      ">acc[MSE]=433.005, acc=0.515, est=0.487, cfg={'max_depth': 6}\n",
      ">acc[MSE]=248.888, acc=0.697, est=0.443, cfg={'max_depth': 6}\n",
      ">acc[MSE]=79.498, acc=0.516, est=0.617, cfg={'max_depth': 6}\n",
      ">acc[MSE]=246.156, acc=0.758, est=0.320, cfg={'max_depth': 6}\n",
      ">acc[MSE]=155.428, acc=0.542, est=0.515, cfg={'max_depth': 6}\n",
      ">acc[MSE]=280.378, acc=0.071, est=0.555, cfg={'max_depth': 6}\n",
      ">acc[MSE]=84.497, acc=0.738, est=0.401, cfg={'max_depth': 6}\n",
      ">acc[MSE]=248.711, acc=0.558, est=0.559, cfg={'max_depth': 6}\n",
      ">acc[MSE]=145.086, acc=0.562, est=0.398, cfg={'max_depth': 6}\n",
      ">acc[MSE]=139.241, acc=0.335, est=0.496, cfg={'max_depth': 6}\n",
      "Accuracy: 206.089 (101.897)\n",
      "Accuracy: 0.529 (0.193)\n"
     ]
    }
   ],
   "source": [
    "#Prepare Abs_Max Target\n",
    "y = c.drop(columns=[\"Name\", \"Abs max\", \"FW80M\"])\n",
    "test_MaxAbs_LR, hat_MaxAbs_LR = NestedCV_LR(X,y, \"FW_LR_test.pckl\", \"FW_LR_hat.pckl\")\n",
    "test_MaxAbs_RF, hat_MaxAbs_RF = NestedCV_RF(X,y, \"MaxAbs_RF_test.pckl\", \"MaxAbs_RF_hat.pckl\")\n",
    "test_MaxAbs_AdaB, hat_MaxAbs_AdaB = NestedCV_AdaB(X,y, \"MaxAbs_AdaB_test.pckl\", \"MaxAbs_AdaB_hat.pckl\")\n",
    "test_MaxAbs_XGB, hat_MaxAbs_XGB = NestedCV_XGB(X,y, \"MaxAbs_XGB_test.pckl\", \"MaxAbs_XGB_hat.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b2ddd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc[MSE]=301.390, acc=0.576, est=-1.385, cfg={}\n",
      ">acc[MSE]=413.010, acc=0.329, est=0.189, cfg={}\n",
      ">acc[MSE]=2510.776, acc=-41.370, est=-0.470, cfg={}\n",
      ">acc[MSE]=535.470, acc=-0.029, est=0.021, cfg={}\n",
      ">acc[MSE]=98.533, acc=0.298, est=-11.777, cfg={}\n",
      ">acc[MSE]=301.017, acc=-0.872, est=-0.274, cfg={}\n",
      ">acc[MSE]=138.861, acc=-0.018, est=-0.136, cfg={}\n",
      ">acc[MSE]=506.122, acc=-0.186, est=-0.397, cfg={}\n",
      ">acc[MSE]=100.123, acc=0.568, est=-0.769, cfg={}\n",
      ">acc[MSE]=75.021, acc=-0.789, est=-0.128, cfg={}\n",
      "Accuracy: 498.032 (690.159)\n",
      "Accuracy: -4.149 (12.416)\n",
      ">acc[MSE]=478.956, acc=0.327, est=0.357, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=292.845, acc=0.524, est=0.351, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=60.394, acc=-0.019, est=0.316, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=164.160, acc=0.685, est=0.140, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=126.650, acc=0.098, est=0.425, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=163.164, acc=-0.015, est=0.482, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=64.544, acc=0.527, est=0.402, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=417.562, acc=0.021, est=0.452, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=84.618, acc=0.635, est=0.157, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=25.145, acc=0.400, est=0.349, cfg={'min_samples_leaf': 4}\n",
      "Accuracy: 187.804 (149.006)\n",
      "Accuracy: 0.318 (0.262)\n",
      ">acc[MSE]=497.831, acc=0.300, est=0.184, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=320.885, acc=0.478, est=-0.014, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=58.246, acc=0.017, est=0.276, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=345.565, acc=0.336, est=0.209, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=200.437, acc=-0.428, est=0.057, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=121.637, acc=0.244, est=0.474, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=28.075, acc=0.794, est=0.283, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=428.980, acc=-0.006, est=0.129, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=41.783, acc=0.820, est=0.359, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=22.172, acc=0.471, est=0.055, cfg={'n_estimators': 300}\n",
      "Accuracy: 206.561 (169.987)\n",
      "Accuracy: 0.303 (0.358)\n",
      ">acc[MSE]=763.451, acc=-0.073, est=0.320, cfg={'max_depth': 6}\n",
      ">acc[MSE]=373.433, acc=0.393, est=0.314, cfg={'max_depth': 6}\n",
      ">acc[MSE]=51.372, acc=0.133, est=0.200, cfg={'max_depth': 6}\n",
      ">acc[MSE]=563.837, acc=-0.084, est=0.005, cfg={'max_depth': 6}\n",
      ">acc[MSE]=155.378, acc=-0.107, est=0.296, cfg={'max_depth': 6}\n",
      ">acc[MSE]=146.296, acc=0.090, est=0.490, cfg={'max_depth': 6}\n",
      ">acc[MSE]=39.137, acc=0.713, est=0.382, cfg={'max_depth': 6}\n",
      ">acc[MSE]=353.254, acc=0.172, est=0.241, cfg={'max_depth': 6}\n",
      ">acc[MSE]=128.660, acc=0.445, est=0.162, cfg={'max_depth': 6}\n",
      ">acc[MSE]=52.113, acc=-0.243, est=-0.006, cfg={'max_depth': 6}\n",
      "Accuracy: 262.693 (233.220)\n",
      "Accuracy: 0.144 (0.281)\n"
     ]
    }
   ],
   "source": [
    "#Prepare FW80M Target\n",
    "y = c.drop(columns=[\"Name\", \"Abs max\", \"Wavelength [nm]\"])\n",
    "test_FW_LR, hat_FW_LR = NestedCV_LR(X,y, \"FW_LR_test.pckl\", \"FW_LR_hat.pckl\")\n",
    "test_FW_RF, hat_FW_RF = NestedCV_RF(X,y, \"FW_RF_test.pckl\", \"FW_RF_hat.pckl\")\n",
    "test_FW_AdaB, hat_FW_AdaB = NestedCV_AdaB(X,y, \"FW_AdaB_test.pckl\", \"FW_AdaB_hat.pckl\")\n",
    "test_FW_XGB, hat_FW_XGB = NestedCV_XGB(X,y, \"FW_XGB_test.pckl\", \"FW_XGB_hat.pckl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7992ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc[MSE]=415.958, acc=0.072, est=-0.159, cfg={}\n",
      ">acc[MSE]=384.832, acc=0.132, est=-1.117, cfg={}\n",
      ">acc[MSE]=871.284, acc=-0.343, est=-0.536, cfg={}\n",
      ">acc[MSE]=488.280, acc=-0.061, est=-1.019, cfg={}\n",
      ">acc[MSE]=148.638, acc=0.296, est=-1.080, cfg={}\n",
      ">acc[MSE]=490.333, acc=-0.108, est=-1.400, cfg={}\n",
      ">acc[MSE]=401.561, acc=0.094, est=-0.233, cfg={}\n",
      ">acc[MSE]=386.812, acc=-2.071, est=-0.040, cfg={}\n",
      ">acc[MSE]=257.915, acc=-0.367, est=-0.189, cfg={}\n",
      ">acc[MSE]=662.736, acc=-0.417, est=-0.153, cfg={}\n",
      "Accuracy: 450.835 (191.124)\n",
      "Accuracy: -0.277 (0.639)\n",
      ">acc[MSE]=288.782, acc=0.356, est=0.305, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=323.674, acc=0.270, est=0.211, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=407.552, acc=0.372, est=0.166, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=371.954, acc=0.192, est=0.162, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=121.708, acc=0.424, est=0.194, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=262.253, acc=0.407, est=0.191, cfg={'min_samples_leaf': 2}\n",
      ">acc[MSE]=186.383, acc=0.579, est=0.107, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=212.743, acc=-0.689, est=0.238, cfg={'min_samples_leaf': 4}\n",
      ">acc[MSE]=327.436, acc=-0.735, est=0.244, cfg={'min_samples_leaf': 1}\n",
      ">acc[MSE]=410.730, acc=0.122, est=0.253, cfg={'min_samples_leaf': 2}\n",
      "Accuracy: 291.321 (91.339)\n",
      "Accuracy: 0.130 (0.438)\n",
      ">acc[MSE]=435.116, acc=0.029, est=0.214, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=196.240, acc=0.557, est=-0.006, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=396.513, acc=0.389, est=0.162, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=454.658, acc=0.012, est=0.268, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=164.444, acc=0.221, est=0.305, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=192.823, acc=0.564, est=0.235, cfg={'n_estimators': 500}\n",
      ">acc[MSE]=182.519, acc=0.588, est=0.266, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=155.087, acc=-0.231, est=0.246, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=446.462, acc=-1.366, est=0.292, cfg={'n_estimators': 300}\n",
      ">acc[MSE]=373.144, acc=0.202, est=0.176, cfg={'n_estimators': 500}\n",
      "Accuracy: 299.700 (123.984)\n",
      "Accuracy: 0.097 (0.552)\n",
      ">acc[MSE]=323.981, acc=0.277, est=0.192, cfg={'max_depth': 6}\n",
      ">acc[MSE]=294.902, acc=0.335, est=0.172, cfg={'max_depth': 6}\n",
      ">acc[MSE]=324.907, acc=0.499, est=-0.077, cfg={'max_depth': 6}\n",
      ">acc[MSE]=588.068, acc=-0.278, est=0.199, cfg={'max_depth': 6}\n",
      ">acc[MSE]=101.975, acc=0.517, est=0.234, cfg={'max_depth': 6}\n",
      ">acc[MSE]=251.558, acc=0.431, est=-0.000, cfg={'max_depth': 6}\n",
      ">acc[MSE]=152.327, acc=0.656, est=-0.099, cfg={'max_depth': 6}\n",
      ">acc[MSE]=285.669, acc=-1.268, est=0.095, cfg={'max_depth': 6}\n",
      ">acc[MSE]=419.822, acc=-1.225, est=0.148, cfg={'max_depth': 6}\n",
      ">acc[MSE]=357.954, acc=0.235, est=0.130, cfg={'max_depth': 6}\n",
      "Accuracy: 310.116 (128.246)\n",
      "Accuracy: 0.018 (0.675)\n"
     ]
    }
   ],
   "source": [
    "#Prepare DLS Target\n",
    "y = a[\"DLS Diameter [nm]\"]\n",
    "test_Dia_LR, hat_Dia_LR = NestedCV_LR(X,y, \"Dia_LR_test.pckl\", \"Dia_LR_hat.pckl\")\n",
    "test_Dia_RF, hat_Dia_RF = NestedCV_RF(X,y, \"Dia_RF_test.pckl\", \"Dia_RF_hat.pckl\")\n",
    "test_Dia_AdaB, hat_Dia_AdaB = NestedCV_AdaB(X,y, \"Dia_AdaB_test.pckl\", \"Dia_AdaB_hat.pckl\")\n",
    "test_Dia_XGB, hat_Dia_XGB = NestedCV_XGB(X,y, \"Dia_XGB_test.pckl\", \"Dia_XGB_hat.pckl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20f59be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.007, est=0.463, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.408, est=0.463, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.002, est=0.413, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.465, est=0.413, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.001, est=0.410, cfg={'min_samples_leaf': 1}\n",
      ">acc=0.770, est=0.410, cfg={'min_samples_leaf': 1}\n",
      ">acc=0.003, est=0.349, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.461, est=0.349, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.003, est=0.455, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.011, est=0.455, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.003, est=0.483, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.045, est=0.483, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.003, est=0.488, cfg={'min_samples_leaf': 2}\n",
      ">acc=-0.818, est=0.488, cfg={'min_samples_leaf': 2}\n",
      ">acc=0.004, est=0.483, cfg={'min_samples_leaf': 4}\n",
      ">acc=0.217, est=0.483, cfg={'min_samples_leaf': 4}\n",
      ">acc=0.004, est=0.281, cfg={'min_samples_leaf': 4}\n",
      ">acc=0.457, est=0.281, cfg={'min_samples_leaf': 4}\n",
      ">acc=0.001, est=0.323, cfg={'min_samples_leaf': 4}\n",
      ">acc=0.661, est=0.323, cfg={'min_samples_leaf': 4}\n",
      "Accuracy: 0.003 (0.002)\n",
      "Accuracy: 0.268 (0.428)\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Full Spectrum\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "outer_results2 = list()\n",
    "y_graph_hat_CombRF = list()\n",
    "y_graph_test_CombRF = list()\n",
    "#Normalize Spectra\n",
    "scaler=MaxAbsScaler()\n",
    "b = b.drop(columns=[\"Wavelength [nm]\"])\n",
    "y = pd.DataFrame(scaler.fit_transform(b.T).T,columns=b.columns)\n",
    "X2 = X.to_numpy()\n",
    "y2 = y.to_numpy()\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "# split data\n",
    "    X_train, X_test = X2[train_ix, :], X2[test_ix, :]\n",
    "    y_train, y_test = y2[train_ix], y2[test_ix]\n",
    "    y_graph_test_CombRF.append(y_test)\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    # define the model\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs = -1)\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['n_estimators'] = [500, 700, 900, 1000, 1100, 1300, 1500]\n",
    "    space['max_features'] = [1.0, 'sqrt', 0.3]\n",
    "    space['min_samples_leaf'] = [1, 2, 4]\n",
    "    space['min_samples_split'] = [2, 4, 8, 16]\n",
    "    # define search\n",
    "    search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    y_graph_hat_CombRF.append(yhat)\n",
    "    # evaluate the model\n",
    "    acc = mean_squared_error(y_test, yhat)\n",
    "    acc2 = r2_score(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    outer_results2.append(acc2)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc2, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(outer_results2), std(outer_results2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85a1c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.609, cfg={'max_features': 0.3, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 1100}\n"
     ]
    }
   ],
   "source": [
    "##Final model training for Random Forest Abs_Max for SHAP Analysis\n",
    "y = c.drop(columns=[\"Name\", \"Abs max\", \"FW80M\"])\n",
    "cv_inner = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# define the model\n",
    "model = RandomForestRegressor(random_state=42, n_jobs = -1)\n",
    "space = dict()\n",
    "space['n_estimators'] = [500, 700, 900, 1000, 1100, 1300, 1500]\n",
    "space['max_features'] = [1.0, 'sqrt', 0.3]\n",
    "space['min_samples_leaf'] = [1, 2, 4]\n",
    "space['min_samples_split'] = [2, 4, 8, 16]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='r2', cv=cv_inner, refit=True, n_jobs=-1)\n",
    "# execute search\n",
    "result = search.fit(X.to_numpy(), y.to_numpy().ravel())\n",
    "print('>acc=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae3a5d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.3, min_samples_split=4, n_estimators=1100,\n",
       "                      n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = search.best_estimator_\n",
    "y = c.drop(columns=[\"Name\", \"Abs max\", \"FW80M\"])\n",
    "model.fit(X, y.to_numpy().ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
